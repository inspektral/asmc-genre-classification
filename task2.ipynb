{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/inspektral/asmc-genre-classification/blob/main/task2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_3_sec = pd.read_csv('data/features_3_sec.csv')\n",
    "features_30_sec = pd.read_csv('data/features_30_sec.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7992, 58)\n",
      "(1998, 58)\n"
     ]
    }
   ],
   "source": [
    "USED_SET = features_3_sec\n",
    "USED_SET = USED_SET.drop(['filename', 'length'], axis=1)\n",
    "\n",
    "train, test = train_test_split(USED_SET, test_size=0.2)\n",
    "\n",
    "print(train.shape)\n",
    "print(test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>47</th>\n",
       "      <th>48</th>\n",
       "      <th>49</th>\n",
       "      <th>50</th>\n",
       "      <th>51</th>\n",
       "      <th>52</th>\n",
       "      <th>53</th>\n",
       "      <th>54</th>\n",
       "      <th>55</th>\n",
       "      <th>56</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.673424</td>\n",
       "      <td>-0.288273</td>\n",
       "      <td>0.729345</td>\n",
       "      <td>0.468020</td>\n",
       "      <td>1.046912</td>\n",
       "      <td>0.954777</td>\n",
       "      <td>1.551684</td>\n",
       "      <td>0.282158</td>\n",
       "      <td>1.349361</td>\n",
       "      <td>0.750730</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.298458</td>\n",
       "      <td>-0.546831</td>\n",
       "      <td>0.525278</td>\n",
       "      <td>-0.134116</td>\n",
       "      <td>0.100966</td>\n",
       "      <td>0.132894</td>\n",
       "      <td>0.760050</td>\n",
       "      <td>-0.068766</td>\n",
       "      <td>0.864039</td>\n",
       "      <td>-0.326858</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.461401</td>\n",
       "      <td>0.163874</td>\n",
       "      <td>-0.336566</td>\n",
       "      <td>-0.302537</td>\n",
       "      <td>-0.586598</td>\n",
       "      <td>0.289117</td>\n",
       "      <td>-0.845288</td>\n",
       "      <td>0.165459</td>\n",
       "      <td>-0.619919</td>\n",
       "      <td>-0.097945</td>\n",
       "      <td>...</td>\n",
       "      <td>0.289999</td>\n",
       "      <td>-0.047633</td>\n",
       "      <td>-1.307722</td>\n",
       "      <td>1.542803</td>\n",
       "      <td>-0.102528</td>\n",
       "      <td>1.353264</td>\n",
       "      <td>-0.019237</td>\n",
       "      <td>0.189018</td>\n",
       "      <td>-0.830446</td>\n",
       "      <td>0.088715</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.435302</td>\n",
       "      <td>-1.001558</td>\n",
       "      <td>1.209553</td>\n",
       "      <td>-0.578434</td>\n",
       "      <td>0.881867</td>\n",
       "      <td>-0.827301</td>\n",
       "      <td>0.184328</td>\n",
       "      <td>-0.990269</td>\n",
       "      <td>0.610584</td>\n",
       "      <td>-0.944590</td>\n",
       "      <td>...</td>\n",
       "      <td>0.856172</td>\n",
       "      <td>-0.728820</td>\n",
       "      <td>-1.044059</td>\n",
       "      <td>-0.839181</td>\n",
       "      <td>0.479815</td>\n",
       "      <td>-0.692576</td>\n",
       "      <td>-0.969211</td>\n",
       "      <td>-0.529100</td>\n",
       "      <td>1.302170</td>\n",
       "      <td>-0.455791</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2.030784</td>\n",
       "      <td>-2.625154</td>\n",
       "      <td>1.495316</td>\n",
       "      <td>-0.387994</td>\n",
       "      <td>0.746575</td>\n",
       "      <td>-0.842783</td>\n",
       "      <td>0.134002</td>\n",
       "      <td>-0.860663</td>\n",
       "      <td>0.393353</td>\n",
       "      <td>-0.924483</td>\n",
       "      <td>...</td>\n",
       "      <td>1.239355</td>\n",
       "      <td>-0.484880</td>\n",
       "      <td>-0.809967</td>\n",
       "      <td>-0.631177</td>\n",
       "      <td>-0.206062</td>\n",
       "      <td>-1.046402</td>\n",
       "      <td>-0.337907</td>\n",
       "      <td>-0.970194</td>\n",
       "      <td>0.038370</td>\n",
       "      <td>-0.884448</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.134561</td>\n",
       "      <td>-0.344592</td>\n",
       "      <td>0.429049</td>\n",
       "      <td>-0.403468</td>\n",
       "      <td>0.291117</td>\n",
       "      <td>-0.603499</td>\n",
       "      <td>1.066304</td>\n",
       "      <td>-0.562648</td>\n",
       "      <td>0.752153</td>\n",
       "      <td>-0.448314</td>\n",
       "      <td>...</td>\n",
       "      <td>0.340602</td>\n",
       "      <td>-0.235873</td>\n",
       "      <td>0.745575</td>\n",
       "      <td>-0.560744</td>\n",
       "      <td>-0.222726</td>\n",
       "      <td>-0.443932</td>\n",
       "      <td>0.122265</td>\n",
       "      <td>-0.530217</td>\n",
       "      <td>0.574004</td>\n",
       "      <td>-0.806890</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7987</th>\n",
       "      <td>0.567410</td>\n",
       "      <td>0.568173</td>\n",
       "      <td>-0.959656</td>\n",
       "      <td>-0.232256</td>\n",
       "      <td>-0.114100</td>\n",
       "      <td>2.222123</td>\n",
       "      <td>0.184988</td>\n",
       "      <td>4.059654</td>\n",
       "      <td>-0.081239</td>\n",
       "      <td>3.553738</td>\n",
       "      <td>...</td>\n",
       "      <td>0.607984</td>\n",
       "      <td>0.160839</td>\n",
       "      <td>0.510437</td>\n",
       "      <td>-0.578681</td>\n",
       "      <td>0.308932</td>\n",
       "      <td>-0.499104</td>\n",
       "      <td>0.675219</td>\n",
       "      <td>0.099482</td>\n",
       "      <td>0.651469</td>\n",
       "      <td>-0.035453</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7988</th>\n",
       "      <td>-1.749245</td>\n",
       "      <td>-0.315360</td>\n",
       "      <td>-0.539120</td>\n",
       "      <td>-0.647198</td>\n",
       "      <td>-2.042341</td>\n",
       "      <td>-0.919103</td>\n",
       "      <td>-1.955043</td>\n",
       "      <td>-0.474263</td>\n",
       "      <td>-2.213618</td>\n",
       "      <td>-1.069972</td>\n",
       "      <td>...</td>\n",
       "      <td>-3.272017</td>\n",
       "      <td>-1.082200</td>\n",
       "      <td>-0.946817</td>\n",
       "      <td>-0.560340</td>\n",
       "      <td>-0.423746</td>\n",
       "      <td>-0.351892</td>\n",
       "      <td>0.951361</td>\n",
       "      <td>-0.423074</td>\n",
       "      <td>-0.715917</td>\n",
       "      <td>-0.524336</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7989</th>\n",
       "      <td>0.116240</td>\n",
       "      <td>-0.038047</td>\n",
       "      <td>0.426530</td>\n",
       "      <td>-0.018381</td>\n",
       "      <td>-0.030562</td>\n",
       "      <td>-0.651359</td>\n",
       "      <td>0.321983</td>\n",
       "      <td>-0.890378</td>\n",
       "      <td>0.492467</td>\n",
       "      <td>-0.751744</td>\n",
       "      <td>...</td>\n",
       "      <td>0.670576</td>\n",
       "      <td>-0.425001</td>\n",
       "      <td>0.379618</td>\n",
       "      <td>0.342696</td>\n",
       "      <td>1.492449</td>\n",
       "      <td>0.298266</td>\n",
       "      <td>-0.794143</td>\n",
       "      <td>0.853098</td>\n",
       "      <td>-0.117725</td>\n",
       "      <td>0.299230</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7990</th>\n",
       "      <td>0.748057</td>\n",
       "      <td>-0.080699</td>\n",
       "      <td>0.587995</td>\n",
       "      <td>-0.454090</td>\n",
       "      <td>1.316712</td>\n",
       "      <td>0.694502</td>\n",
       "      <td>1.139679</td>\n",
       "      <td>-0.457870</td>\n",
       "      <td>1.262934</td>\n",
       "      <td>0.088799</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.188605</td>\n",
       "      <td>-0.800423</td>\n",
       "      <td>0.416360</td>\n",
       "      <td>-0.880185</td>\n",
       "      <td>-0.691473</td>\n",
       "      <td>-0.453745</td>\n",
       "      <td>0.553620</td>\n",
       "      <td>-0.704717</td>\n",
       "      <td>0.721939</td>\n",
       "      <td>-0.435217</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7991</th>\n",
       "      <td>0.791569</td>\n",
       "      <td>0.396608</td>\n",
       "      <td>1.343378</td>\n",
       "      <td>2.017785</td>\n",
       "      <td>0.642830</td>\n",
       "      <td>3.068162</td>\n",
       "      <td>1.329013</td>\n",
       "      <td>0.067892</td>\n",
       "      <td>0.968575</td>\n",
       "      <td>0.915140</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.609470</td>\n",
       "      <td>-0.286538</td>\n",
       "      <td>0.503427</td>\n",
       "      <td>-0.481909</td>\n",
       "      <td>-0.099124</td>\n",
       "      <td>0.087094</td>\n",
       "      <td>0.172000</td>\n",
       "      <td>0.605935</td>\n",
       "      <td>-1.302464</td>\n",
       "      <td>0.110035</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7992 rows Ã— 57 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            0         1         2         3         4         5         6   \\\n",
       "0    -0.673424 -0.288273  0.729345  0.468020  1.046912  0.954777  1.551684   \n",
       "1    -0.461401  0.163874 -0.336566 -0.302537 -0.586598  0.289117 -0.845288   \n",
       "2     1.435302 -1.001558  1.209553 -0.578434  0.881867 -0.827301  0.184328   \n",
       "3     2.030784 -2.625154  1.495316 -0.387994  0.746575 -0.842783  0.134002   \n",
       "4     0.134561 -0.344592  0.429049 -0.403468  0.291117 -0.603499  1.066304   \n",
       "...        ...       ...       ...       ...       ...       ...       ...   \n",
       "7987  0.567410  0.568173 -0.959656 -0.232256 -0.114100  2.222123  0.184988   \n",
       "7988 -1.749245 -0.315360 -0.539120 -0.647198 -2.042341 -0.919103 -1.955043   \n",
       "7989  0.116240 -0.038047  0.426530 -0.018381 -0.030562 -0.651359  0.321983   \n",
       "7990  0.748057 -0.080699  0.587995 -0.454090  1.316712  0.694502  1.139679   \n",
       "7991  0.791569  0.396608  1.343378  2.017785  0.642830  3.068162  1.329013   \n",
       "\n",
       "            7         8         9   ...        47        48        49  \\\n",
       "0     0.282158  1.349361  0.750730  ... -0.298458 -0.546831  0.525278   \n",
       "1     0.165459 -0.619919 -0.097945  ...  0.289999 -0.047633 -1.307722   \n",
       "2    -0.990269  0.610584 -0.944590  ...  0.856172 -0.728820 -1.044059   \n",
       "3    -0.860663  0.393353 -0.924483  ...  1.239355 -0.484880 -0.809967   \n",
       "4    -0.562648  0.752153 -0.448314  ...  0.340602 -0.235873  0.745575   \n",
       "...        ...       ...       ...  ...       ...       ...       ...   \n",
       "7987  4.059654 -0.081239  3.553738  ...  0.607984  0.160839  0.510437   \n",
       "7988 -0.474263 -2.213618 -1.069972  ... -3.272017 -1.082200 -0.946817   \n",
       "7989 -0.890378  0.492467 -0.751744  ...  0.670576 -0.425001  0.379618   \n",
       "7990 -0.457870  1.262934  0.088799  ... -1.188605 -0.800423  0.416360   \n",
       "7991  0.067892  0.968575  0.915140  ... -0.609470 -0.286538  0.503427   \n",
       "\n",
       "            50        51        52        53        54        55        56  \n",
       "0    -0.134116  0.100966  0.132894  0.760050 -0.068766  0.864039 -0.326858  \n",
       "1     1.542803 -0.102528  1.353264 -0.019237  0.189018 -0.830446  0.088715  \n",
       "2    -0.839181  0.479815 -0.692576 -0.969211 -0.529100  1.302170 -0.455791  \n",
       "3    -0.631177 -0.206062 -1.046402 -0.337907 -0.970194  0.038370 -0.884448  \n",
       "4    -0.560744 -0.222726 -0.443932  0.122265 -0.530217  0.574004 -0.806890  \n",
       "...        ...       ...       ...       ...       ...       ...       ...  \n",
       "7987 -0.578681  0.308932 -0.499104  0.675219  0.099482  0.651469 -0.035453  \n",
       "7988 -0.560340 -0.423746 -0.351892  0.951361 -0.423074 -0.715917 -0.524336  \n",
       "7989  0.342696  1.492449  0.298266 -0.794143  0.853098 -0.117725  0.299230  \n",
       "7990 -0.880185 -0.691473 -0.453745  0.553620 -0.704717  0.721939 -0.435217  \n",
       "7991 -0.481909 -0.099124  0.087094  0.172000  0.605935 -1.302464  0.110035  \n",
       "\n",
       "[7992 rows x 57 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Normalize the data\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "\n",
    "def normalize_feature_set(features):\n",
    "    scaler = StandardScaler()\n",
    "\n",
    "    scaler.fit(features)\n",
    "    scaled_features = scaler.transform(features)\n",
    "\n",
    "    scaled_df = pd.DataFrame(scaled_features)\n",
    "\n",
    "    return scaled_df\n",
    "\n",
    "\n",
    "X_test = test.drop('label', axis=1)\n",
    "X_test = normalize_feature_set(X_test)\n",
    "y_test = test['label']\n",
    "\n",
    "X_train = train.drop('label', axis=1)\n",
    "X_train = normalize_feature_set(X_train)\n",
    "y_train = train['label']\n",
    "\n",
    "labelencoder = LabelEncoder()\n",
    "\n",
    "y_train = labelencoder.fit_transform(y_train)\n",
    "y_test = labelencoder.fit_transform(y_test)\n",
    "\n",
    "display(X_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import f_classif\n",
    "\n",
    "selector = SelectKBest(f_classif, k=30)\n",
    "selector.fit(X_train, y_train)\n",
    "\n",
    "X_train = selector.transform(X_train)\n",
    "X_test = selector.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8228228228228228\n"
     ]
    }
   ],
   "source": [
    "# MLP Classifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "mlp = MLPClassifier(hidden_layer_sizes=(100, 100, 100), max_iter=1000)\n",
    "mlp.fit(X_train, y_train)\n",
    "\n",
    "y_pred = mlp.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8558558558558559\n"
     ]
    }
   ],
   "source": [
    "# kNN classifier\n",
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "knn = KNeighborsClassifier(n_neighbors=5)\n",
    "\n",
    "knn.fit(X_train, y_train)\n",
    "\n",
    "predictions = knn.predict(X_test)\n",
    "\n",
    "accuracy = accuracy_score(y_test, predictions)\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7197197197197197\n"
     ]
    }
   ],
   "source": [
    "# SVM classifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "svm = SVC(kernel='linear')\n",
    "svm.fit(X_train, y_train)\n",
    "\n",
    "y_pred = svm.predict(X_test)\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8228228228228228\n"
     ]
    }
   ],
   "source": [
    "# Random Forest classifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "rf = RandomForestClassifier(n_estimators=100)\n",
    "rf.fit(X_train, y_train)\n",
    "\n",
    "predictions = rf.predict(X_test)\n",
    "\n",
    "accuracy = accuracy_score(y_test, predictions)\n",
    "print(accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6781781781781782\n"
     ]
    }
   ],
   "source": [
    "# Logistic Regression\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "lr = LogisticRegression(max_iter=1000)\n",
    "lr.fit(X_train, y_train)\n",
    "\n",
    "predictions = lr.predict(X_test)\n",
    "\n",
    "accuracy = accuracy_score(y_test, predictions)\n",
    "print(accuracy)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mir",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
