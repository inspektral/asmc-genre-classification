{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/inspektral/asmc-genre-classification/blob/main/task2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_3_sec = pd.read_csv('data/features_3_sec.csv')\n",
    "features_30_sec = pd.read_csv('data/features_30_sec.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7992, 58)\n",
      "(1998, 58)\n"
     ]
    }
   ],
   "source": [
    "USED_SET = features_3_sec\n",
    "USED_SET = USED_SET.drop(['filename', 'length'], axis=1)\n",
    "\n",
    "train, test = train_test_split(USED_SET, test_size=0.2)\n",
    "\n",
    "print(train.shape)\n",
    "print(test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>47</th>\n",
       "      <th>48</th>\n",
       "      <th>49</th>\n",
       "      <th>50</th>\n",
       "      <th>51</th>\n",
       "      <th>52</th>\n",
       "      <th>53</th>\n",
       "      <th>54</th>\n",
       "      <th>55</th>\n",
       "      <th>56</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.326675</td>\n",
       "      <td>-0.810086</td>\n",
       "      <td>1.461030</td>\n",
       "      <td>0.399177</td>\n",
       "      <td>1.820027</td>\n",
       "      <td>-0.280683</td>\n",
       "      <td>2.190014</td>\n",
       "      <td>-0.753905</td>\n",
       "      <td>2.229382</td>\n",
       "      <td>-0.805124</td>\n",
       "      <td>...</td>\n",
       "      <td>0.777428</td>\n",
       "      <td>0.758329</td>\n",
       "      <td>1.268108</td>\n",
       "      <td>0.245607</td>\n",
       "      <td>-0.010867</td>\n",
       "      <td>-0.020851</td>\n",
       "      <td>0.787519</td>\n",
       "      <td>-0.280104</td>\n",
       "      <td>-0.012803</td>\n",
       "      <td>-0.738679</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-2.010185</td>\n",
       "      <td>-0.856363</td>\n",
       "      <td>0.716608</td>\n",
       "      <td>-0.517306</td>\n",
       "      <td>-0.948875</td>\n",
       "      <td>-0.412166</td>\n",
       "      <td>-0.395745</td>\n",
       "      <td>2.827436</td>\n",
       "      <td>-0.896480</td>\n",
       "      <td>0.638306</td>\n",
       "      <td>...</td>\n",
       "      <td>0.401814</td>\n",
       "      <td>-0.735883</td>\n",
       "      <td>2.660683</td>\n",
       "      <td>-0.789826</td>\n",
       "      <td>2.590937</td>\n",
       "      <td>-0.686658</td>\n",
       "      <td>2.782969</td>\n",
       "      <td>-0.055644</td>\n",
       "      <td>0.818849</td>\n",
       "      <td>-0.654798</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.947290</td>\n",
       "      <td>-1.816447</td>\n",
       "      <td>0.220929</td>\n",
       "      <td>-0.495898</td>\n",
       "      <td>0.692822</td>\n",
       "      <td>-0.550861</td>\n",
       "      <td>-0.213200</td>\n",
       "      <td>-0.794271</td>\n",
       "      <td>0.299785</td>\n",
       "      <td>-0.723970</td>\n",
       "      <td>...</td>\n",
       "      <td>0.716999</td>\n",
       "      <td>-0.599275</td>\n",
       "      <td>-0.616239</td>\n",
       "      <td>-0.335537</td>\n",
       "      <td>0.665298</td>\n",
       "      <td>-0.441538</td>\n",
       "      <td>0.068234</td>\n",
       "      <td>-0.460460</td>\n",
       "      <td>0.945014</td>\n",
       "      <td>-0.718941</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.813428</td>\n",
       "      <td>0.147034</td>\n",
       "      <td>0.656217</td>\n",
       "      <td>-0.153235</td>\n",
       "      <td>-0.276984</td>\n",
       "      <td>-0.499237</td>\n",
       "      <td>-0.312503</td>\n",
       "      <td>-0.650609</td>\n",
       "      <td>-0.159326</td>\n",
       "      <td>-0.602336</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.041712</td>\n",
       "      <td>-0.543970</td>\n",
       "      <td>0.031990</td>\n",
       "      <td>-0.811266</td>\n",
       "      <td>1.005799</td>\n",
       "      <td>-0.493737</td>\n",
       "      <td>0.070190</td>\n",
       "      <td>-0.972061</td>\n",
       "      <td>0.095535</td>\n",
       "      <td>-0.677387</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.636067</td>\n",
       "      <td>-2.362087</td>\n",
       "      <td>0.935830</td>\n",
       "      <td>-0.655355</td>\n",
       "      <td>0.782940</td>\n",
       "      <td>-0.867751</td>\n",
       "      <td>0.099695</td>\n",
       "      <td>-0.984171</td>\n",
       "      <td>0.416122</td>\n",
       "      <td>-0.963871</td>\n",
       "      <td>...</td>\n",
       "      <td>0.706987</td>\n",
       "      <td>-0.944007</td>\n",
       "      <td>-1.290714</td>\n",
       "      <td>-0.990446</td>\n",
       "      <td>0.096672</td>\n",
       "      <td>-0.772600</td>\n",
       "      <td>-0.130090</td>\n",
       "      <td>-0.813534</td>\n",
       "      <td>1.705363</td>\n",
       "      <td>-0.711993</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7987</th>\n",
       "      <td>1.770445</td>\n",
       "      <td>-1.475779</td>\n",
       "      <td>1.155623</td>\n",
       "      <td>-0.218586</td>\n",
       "      <td>0.502180</td>\n",
       "      <td>-0.106968</td>\n",
       "      <td>-0.020890</td>\n",
       "      <td>0.141239</td>\n",
       "      <td>0.263924</td>\n",
       "      <td>-0.228299</td>\n",
       "      <td>...</td>\n",
       "      <td>0.753216</td>\n",
       "      <td>-0.632543</td>\n",
       "      <td>-0.074821</td>\n",
       "      <td>-0.765302</td>\n",
       "      <td>0.117474</td>\n",
       "      <td>-0.969783</td>\n",
       "      <td>-0.695358</td>\n",
       "      <td>-0.783136</td>\n",
       "      <td>1.495425</td>\n",
       "      <td>-0.895576</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7988</th>\n",
       "      <td>0.937707</td>\n",
       "      <td>-1.071436</td>\n",
       "      <td>0.668398</td>\n",
       "      <td>-0.165964</td>\n",
       "      <td>-0.100532</td>\n",
       "      <td>-0.107395</td>\n",
       "      <td>0.083619</td>\n",
       "      <td>-0.548573</td>\n",
       "      <td>0.163329</td>\n",
       "      <td>-0.385613</td>\n",
       "      <td>...</td>\n",
       "      <td>0.823334</td>\n",
       "      <td>-1.067657</td>\n",
       "      <td>-0.581318</td>\n",
       "      <td>-0.700958</td>\n",
       "      <td>-0.155848</td>\n",
       "      <td>-0.808539</td>\n",
       "      <td>-1.424395</td>\n",
       "      <td>-0.620735</td>\n",
       "      <td>-0.473372</td>\n",
       "      <td>-0.777539</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7989</th>\n",
       "      <td>0.361908</td>\n",
       "      <td>1.202026</td>\n",
       "      <td>-0.008549</td>\n",
       "      <td>0.555793</td>\n",
       "      <td>-0.441428</td>\n",
       "      <td>-0.574171</td>\n",
       "      <td>-1.124785</td>\n",
       "      <td>-0.896092</td>\n",
       "      <td>-0.615722</td>\n",
       "      <td>-0.999435</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.762188</td>\n",
       "      <td>0.109195</td>\n",
       "      <td>0.447152</td>\n",
       "      <td>0.056415</td>\n",
       "      <td>-0.320720</td>\n",
       "      <td>-0.222549</td>\n",
       "      <td>0.218289</td>\n",
       "      <td>-0.421315</td>\n",
       "      <td>-1.428951</td>\n",
       "      <td>-0.135731</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7990</th>\n",
       "      <td>2.076519</td>\n",
       "      <td>-1.680569</td>\n",
       "      <td>1.274730</td>\n",
       "      <td>0.154482</td>\n",
       "      <td>0.258214</td>\n",
       "      <td>-0.355632</td>\n",
       "      <td>-0.122066</td>\n",
       "      <td>-0.412966</td>\n",
       "      <td>0.150504</td>\n",
       "      <td>-0.456433</td>\n",
       "      <td>...</td>\n",
       "      <td>0.713584</td>\n",
       "      <td>-0.539195</td>\n",
       "      <td>-0.245503</td>\n",
       "      <td>-0.624990</td>\n",
       "      <td>1.274379</td>\n",
       "      <td>-0.792975</td>\n",
       "      <td>-0.021794</td>\n",
       "      <td>-0.724270</td>\n",
       "      <td>0.943705</td>\n",
       "      <td>-0.759224</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7991</th>\n",
       "      <td>0.039490</td>\n",
       "      <td>-0.270955</td>\n",
       "      <td>0.717146</td>\n",
       "      <td>0.256341</td>\n",
       "      <td>0.499673</td>\n",
       "      <td>-0.326691</td>\n",
       "      <td>0.615048</td>\n",
       "      <td>-0.687750</td>\n",
       "      <td>0.823176</td>\n",
       "      <td>-0.623761</td>\n",
       "      <td>...</td>\n",
       "      <td>0.649555</td>\n",
       "      <td>-0.553145</td>\n",
       "      <td>-0.602373</td>\n",
       "      <td>-0.759262</td>\n",
       "      <td>1.674514</td>\n",
       "      <td>-0.633889</td>\n",
       "      <td>-0.961338</td>\n",
       "      <td>-0.202238</td>\n",
       "      <td>0.351480</td>\n",
       "      <td>-0.468761</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7992 rows Ã— 57 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            0         1         2         3         4         5         6   \\\n",
       "0     0.326675 -0.810086  1.461030  0.399177  1.820027 -0.280683  2.190014   \n",
       "1    -2.010185 -0.856363  0.716608 -0.517306 -0.948875 -0.412166 -0.395745   \n",
       "2     1.947290 -1.816447  0.220929 -0.495898  0.692822 -0.550861 -0.213200   \n",
       "3    -0.813428  0.147034  0.656217 -0.153235 -0.276984 -0.499237 -0.312503   \n",
       "4     0.636067 -2.362087  0.935830 -0.655355  0.782940 -0.867751  0.099695   \n",
       "...        ...       ...       ...       ...       ...       ...       ...   \n",
       "7987  1.770445 -1.475779  1.155623 -0.218586  0.502180 -0.106968 -0.020890   \n",
       "7988  0.937707 -1.071436  0.668398 -0.165964 -0.100532 -0.107395  0.083619   \n",
       "7989  0.361908  1.202026 -0.008549  0.555793 -0.441428 -0.574171 -1.124785   \n",
       "7990  2.076519 -1.680569  1.274730  0.154482  0.258214 -0.355632 -0.122066   \n",
       "7991  0.039490 -0.270955  0.717146  0.256341  0.499673 -0.326691  0.615048   \n",
       "\n",
       "            7         8         9   ...        47        48        49  \\\n",
       "0    -0.753905  2.229382 -0.805124  ...  0.777428  0.758329  1.268108   \n",
       "1     2.827436 -0.896480  0.638306  ...  0.401814 -0.735883  2.660683   \n",
       "2    -0.794271  0.299785 -0.723970  ...  0.716999 -0.599275 -0.616239   \n",
       "3    -0.650609 -0.159326 -0.602336  ... -0.041712 -0.543970  0.031990   \n",
       "4    -0.984171  0.416122 -0.963871  ...  0.706987 -0.944007 -1.290714   \n",
       "...        ...       ...       ...  ...       ...       ...       ...   \n",
       "7987  0.141239  0.263924 -0.228299  ...  0.753216 -0.632543 -0.074821   \n",
       "7988 -0.548573  0.163329 -0.385613  ...  0.823334 -1.067657 -0.581318   \n",
       "7989 -0.896092 -0.615722 -0.999435  ... -0.762188  0.109195  0.447152   \n",
       "7990 -0.412966  0.150504 -0.456433  ...  0.713584 -0.539195 -0.245503   \n",
       "7991 -0.687750  0.823176 -0.623761  ...  0.649555 -0.553145 -0.602373   \n",
       "\n",
       "            50        51        52        53        54        55        56  \n",
       "0     0.245607 -0.010867 -0.020851  0.787519 -0.280104 -0.012803 -0.738679  \n",
       "1    -0.789826  2.590937 -0.686658  2.782969 -0.055644  0.818849 -0.654798  \n",
       "2    -0.335537  0.665298 -0.441538  0.068234 -0.460460  0.945014 -0.718941  \n",
       "3    -0.811266  1.005799 -0.493737  0.070190 -0.972061  0.095535 -0.677387  \n",
       "4    -0.990446  0.096672 -0.772600 -0.130090 -0.813534  1.705363 -0.711993  \n",
       "...        ...       ...       ...       ...       ...       ...       ...  \n",
       "7987 -0.765302  0.117474 -0.969783 -0.695358 -0.783136  1.495425 -0.895576  \n",
       "7988 -0.700958 -0.155848 -0.808539 -1.424395 -0.620735 -0.473372 -0.777539  \n",
       "7989  0.056415 -0.320720 -0.222549  0.218289 -0.421315 -1.428951 -0.135731  \n",
       "7990 -0.624990  1.274379 -0.792975 -0.021794 -0.724270  0.943705 -0.759224  \n",
       "7991 -0.759262  1.674514 -0.633889 -0.961338 -0.202238  0.351480 -0.468761  \n",
       "\n",
       "[7992 rows x 57 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Normalize the data\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "\n",
    "def normalize_feature_set(features):\n",
    "    scaler = StandardScaler()\n",
    "\n",
    "    scaler.fit(features)\n",
    "    scaled_features = scaler.transform(features)\n",
    "\n",
    "    scaled_df = pd.DataFrame(scaled_features)\n",
    "\n",
    "    return scaled_df\n",
    "\n",
    "\n",
    "X_test = test.drop('label', axis=1)\n",
    "X_test = normalize_feature_set(X_test)\n",
    "y_test = test['label']\n",
    "\n",
    "X_train = train.drop('label', axis=1)\n",
    "X_train = normalize_feature_set(X_train)\n",
    "y_train = train['label']\n",
    "\n",
    "labelencoder = LabelEncoder()\n",
    "\n",
    "y_train = labelencoder.fit_transform(y_train)\n",
    "y_test = labelencoder.fit_transform(y_test)\n",
    "\n",
    "display(X_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import f_classif\n",
    "\n",
    "selector = SelectKBest(f_classif, k=30)\n",
    "selector.fit(X_train, y_train)\n",
    "\n",
    "X_train = selector.transform(X_train)\n",
    "X_test = selector.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7797797797797797\n"
     ]
    }
   ],
   "source": [
    "# MLP Classifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "mlp = MLPClassifier(hidden_layer_sizes=(100, 100, 100), max_iter=1000)\n",
    "mlp.fit(X_train, y_train)\n",
    "\n",
    "y_pred = mlp.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7322322322322322\n"
     ]
    }
   ],
   "source": [
    "# kNN classifier\n",
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "knn = KNeighborsClassifier(n_neighbors=5)\n",
    "\n",
    "knn.fit(X_train, y_train)\n",
    "\n",
    "predictions = knn.predict(X_test)\n",
    "\n",
    "accuracy = accuracy_score(y_test, predictions)\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.561061061061061\n"
     ]
    }
   ],
   "source": [
    "# SVM classifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "svm = SVC(kernel='linear')\n",
    "svm.fit(X_train, y_train)\n",
    "\n",
    "y_pred = svm.predict(X_test)\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7592592592592593\n"
     ]
    }
   ],
   "source": [
    "# Random Forest classifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "rf = RandomForestClassifier(n_estimators=100)\n",
    "rf.fit(X_train, y_train)\n",
    "\n",
    "predictions = rf.predict(X_test)\n",
    "\n",
    "accuracy = accuracy_score(y_test, predictions)\n",
    "print(accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5375375375375375\n"
     ]
    }
   ],
   "source": [
    "# Logistic Regression\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "lr = LogisticRegression(max_iter=1000)\n",
    "lr.fit(X_train, y_train)\n",
    "\n",
    "predictions = lr.predict(X_test)\n",
    "\n",
    "accuracy = accuracy_score(y_test, predictions)\n",
    "print(accuracy)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mir",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
